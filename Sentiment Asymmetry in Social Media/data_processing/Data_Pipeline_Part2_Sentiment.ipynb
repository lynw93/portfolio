{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zhGE4hPuNI6f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6EdyzfbCNI6k"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Test Data/tweet_only_Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dxGgqan9NI6k",
    "outputId": "a417a53d-8c1e-455f-dc4e-f7b52f829826"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Use GPU to run\n",
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1635954265351921667</td>\n",
       "      <td>@TonyGigi2 @HellsKitchenFOX @opensea @GordonRa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1641763189162483715</td>\n",
       "      <td>Please block me before 9pm. üôèüôèüôè \\n\\nFor your o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1636766862414430225</td>\n",
       "      <td>@stockx @Nike 6/6\\n\\n#Phygital NFTs have the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1634232094766563328</td>\n",
       "      <td>@Leecanskate Thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1638952055866794007</td>\n",
       "      <td>@ggreenwald His last name is really Langley???...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              tweet\n",
       "0  1635954265351921667  @TonyGigi2 @HellsKitchenFOX @opensea @GordonRa...\n",
       "1  1641763189162483715  Please block me before 9pm. üôèüôèüôè \\n\\nFor your o...\n",
       "2  1636766862414430225  @stockx @Nike 6/6\\n\\n#Phygital NFTs have the p...\n",
       "3  1634232094766563328                             @Leecanskate Thank you\n",
       "4  1638952055866794007  @ggreenwald His last name is really Langley???..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mRYawxIENgft",
    "outputId": "2822c0ee-e507-4730-fb6f-8d9ef376a597"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# BERT_sentiment_calc\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "from scipy.special import softmax\n",
    "\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(device)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def split_text_to_chunks(text, max_chunk_tokens=510):\n",
    "    \"\"\"Split text into chunks of tokens (max 510 tokens per chunk).\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for token in tokens:\n",
    "        current_chunk.append(token)\n",
    "        if len(current_chunk) >= max_chunk_tokens:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = []\n",
    "\n",
    "    if current_chunk:  # Add the last chunk\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def get_sentiment_for_chunk(chunk_tokens):\n",
    "    text = tokenizer.convert_tokens_to_string(chunk_tokens)  # Convert tokens to string\n",
    "    text = preprocess(text)  # Preprocess the reconstructed string\n",
    "    encoded_input = tokenizer(text, return_tensors='pt').to(model.device)  # Send to correct device\n",
    "    output = model(**encoded_input)\n",
    "    scores = output.logits[0].detach().cpu().numpy()  # safer to access logits explicitly\n",
    "    return softmax(scores)\n",
    "\n",
    "def get_sentiment_for_tweet(tweet):\n",
    "    \"\"\"Simulated processing logic.\"\"\"\n",
    "    chunks = split_text_to_chunks(tweet)\n",
    "    if not chunks:\n",
    "        return None\n",
    "    list_score = [get_sentiment_for_chunk(chunk) for chunk in chunks]\n",
    "    return [sum(col) / len(col) for col in zip(*list_score)]   # Average score\n",
    "\n",
    "# def get_sentiment_for_tweet(tweet): # weighted average - weights are based on the max deviation from neutral (intensity)\n",
    "#     chunks = split_text_to_chunks(tweet)\n",
    "#     if not chunks:\n",
    "#         return None\n",
    "\n",
    "#     list_score = [get_sentiment_for_chunk(chunk) for chunk in chunks]\n",
    "\n",
    "#     # Calculate intensity (distance from neutral)\n",
    "#     intensities = [abs(score[0] - 0.5) + abs(score[2] - 0.5) for score in list_score]\n",
    "#     total_intensity = sum(intensities)\n",
    "#     if total_intensity == 0:\n",
    "#         weights = [1 / len(list_score)] * len(list_score)  # fallback to average\n",
    "#     else:\n",
    "#         weights = [i / total_intensity for i in intensities]\n",
    "\n",
    "#     # Weighted average\n",
    "#     weighted_score = [0.0] * len(list_score[0])\n",
    "#     for w, score in zip(weights, list_score):\n",
    "#         for i in range(len(score)):\n",
    "#             weighted_score[i] += w * score[i]\n",
    "\n",
    "#     return weighted_score\n",
    "\n",
    "# Adding wrapper to count and print progress\n",
    "def sentiment_wrapper(tweet):\n",
    "    global processed_counter\n",
    "    processed_counter += 1\n",
    "    if processed_counter % 1000 == 0:\n",
    "        print(f\"[Progress] Processed {processed_counter} rows.\")\n",
    "    return get_sentiment_for_tweet(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Progress] Processed 1000 rows.\n",
      "[Progress] Processed 2000 rows.\n",
      "[Progress] Processed 3000 rows.\n",
      "[Progress] Processed 4000 rows.\n",
      "[Progress] Processed 5000 rows.\n",
      "[Progress] Processed 6000 rows.\n",
      "[Progress] Processed 7000 rows.\n",
      "[Progress] Processed 8000 rows.\n",
      "[Progress] Processed 9000 rows.\n",
      "[Progress] Processed 10000 rows.\n",
      "[Progress] Processed 11000 rows.\n",
      "[Progress] Processed 12000 rows.\n",
      "[Progress] Processed 13000 rows.\n",
      "[Progress] Processed 14000 rows.\n",
      "[Progress] Processed 15000 rows.\n",
      "[Progress] Processed 16000 rows.\n",
      "[Progress] Processed 17000 rows.\n",
      "[Progress] Processed 18000 rows.\n",
      "[Progress] Processed 19000 rows.\n",
      "[Progress] Processed 20000 rows.\n",
      "[Progress] Processed 21000 rows.\n",
      "[Progress] Processed 22000 rows.\n",
      "[Progress] Processed 23000 rows.\n",
      "[Progress] Processed 24000 rows.\n",
      "[Progress] Processed 25000 rows.\n",
      "[Progress] Processed 26000 rows.\n",
      "[Progress] Processed 27000 rows.\n",
      "[Progress] Processed 28000 rows.\n",
      "[Progress] Processed 29000 rows.\n",
      "[Progress] Processed 30000 rows.\n",
      "[Progress] Processed 31000 rows.\n",
      "[Progress] Processed 32000 rows.\n",
      "[Progress] Processed 33000 rows.\n",
      "[Progress] Processed 34000 rows.\n",
      "[Progress] Processed 35000 rows.\n",
      "[Progress] Processed 36000 rows.\n",
      "[Progress] Processed 37000 rows.\n",
      "[Progress] Processed 38000 rows.\n",
      "[Progress] Processed 39000 rows.\n",
      "[Progress] Processed 40000 rows.\n",
      "[Progress] Processed 41000 rows.\n",
      "[Progress] Processed 42000 rows.\n",
      "[Progress] Processed 43000 rows.\n",
      "[Progress] Processed 44000 rows.\n",
      "[Progress] Processed 45000 rows.\n",
      "[Progress] Processed 46000 rows.\n",
      "[Progress] Processed 47000 rows.\n",
      "[Progress] Processed 48000 rows.\n",
      "[Progress] Processed 49000 rows.\n",
      "[Progress] Processed 50000 rows.\n",
      "[Progress] Processed 51000 rows.\n",
      "[Progress] Processed 52000 rows.\n",
      "[Progress] Processed 53000 rows.\n",
      "[Progress] Processed 54000 rows.\n",
      "[Progress] Processed 55000 rows.\n",
      "[Progress] Processed 56000 rows.\n",
      "[Progress] Processed 57000 rows.\n",
      "[Progress] Processed 58000 rows.\n",
      "[Progress] Processed 59000 rows.\n",
      "[Progress] Processed 60000 rows.\n",
      "[Progress] Processed 61000 rows.\n",
      "[Progress] Processed 62000 rows.\n",
      "[Progress] Processed 63000 rows.\n",
      "[Progress] Processed 64000 rows.\n",
      "[Progress] Processed 65000 rows.\n",
      "[Progress] Processed 66000 rows.\n",
      "[Progress] Processed 67000 rows.\n",
      "[Progress] Processed 68000 rows.\n",
      "[Progress] Processed 69000 rows.\n",
      "[Progress] Processed 70000 rows.\n",
      "[Progress] Processed 71000 rows.\n",
      "[Progress] Processed 72000 rows.\n",
      "[Progress] Processed 73000 rows.\n",
      "[Progress] Processed 74000 rows.\n",
      "[Progress] Processed 75000 rows.\n",
      "[Progress] Processed 76000 rows.\n",
      "[Progress] Processed 77000 rows.\n",
      "[Progress] Processed 78000 rows.\n",
      "[Progress] Processed 79000 rows.\n",
      "[Progress] Processed 80000 rows.\n",
      "[Progress] Processed 81000 rows.\n",
      "[Progress] Processed 82000 rows.\n",
      "[Progress] Processed 83000 rows.\n",
      "[Progress] Processed 84000 rows.\n",
      "[Progress] Processed 85000 rows.\n",
      "[Progress] Processed 86000 rows.\n",
      "[Progress] Processed 87000 rows.\n",
      "[Progress] Processed 88000 rows.\n",
      "[Progress] Processed 89000 rows.\n",
      "[Progress] Processed 90000 rows.\n",
      "[Progress] Processed 91000 rows.\n",
      "[Progress] Processed 92000 rows.\n",
      "[Progress] Processed 93000 rows.\n",
      "[Progress] Processed 94000 rows.\n",
      "[Progress] Processed 95000 rows.\n",
      "[Progress] Processed 96000 rows.\n",
      "[Progress] Processed 97000 rows.\n",
      "[Progress] Processed 98000 rows.\n",
      "[Progress] Processed 99000 rows.\n",
      "[Progress] Processed 100000 rows.\n",
      "[Progress] Processed 101000 rows.\n",
      "[Progress] Processed 102000 rows.\n",
      "[Progress] Processed 103000 rows.\n",
      "[Progress] Processed 104000 rows.\n",
      "[Progress] Processed 105000 rows.\n",
      "[Progress] Processed 106000 rows.\n",
      "[Progress] Processed 107000 rows.\n",
      "[Progress] Processed 108000 rows.\n",
      "[Progress] Processed 109000 rows.\n",
      "[Progress] Processed 110000 rows.\n",
      "[Progress] Processed 111000 rows.\n",
      "[Progress] Processed 112000 rows.\n",
      "[Progress] Processed 113000 rows.\n",
      "[Progress] Processed 114000 rows.\n",
      "[Progress] Processed 115000 rows.\n",
      "[Progress] Processed 116000 rows.\n",
      "[Progress] Processed 117000 rows.\n",
      "[Progress] Processed 118000 rows.\n",
      "[Progress] Processed 119000 rows.\n",
      "[Progress] Processed 120000 rows.\n",
      "[Progress] Processed 121000 rows.\n",
      "[Progress] Processed 122000 rows.\n",
      "[Progress] Processed 123000 rows.\n",
      "[Progress] Processed 124000 rows.\n",
      "[Progress] Processed 125000 rows.\n",
      "[Progress] Processed 126000 rows.\n",
      "[Progress] Processed 127000 rows.\n",
      "[Progress] Processed 128000 rows.\n",
      "[Progress] Processed 129000 rows.\n",
      "[Progress] Processed 130000 rows.\n",
      "[Progress] Processed 131000 rows.\n",
      "[Progress] Processed 132000 rows.\n",
      "[Progress] Processed 133000 rows.\n",
      "[Progress] Processed 134000 rows.\n",
      "[Progress] Processed 135000 rows.\n",
      "[Progress] Processed 136000 rows.\n",
      "[Progress] Processed 137000 rows.\n",
      "[Progress] Processed 138000 rows.\n",
      "[Progress] Processed 139000 rows.\n",
      "[Progress] Processed 140000 rows.\n",
      "[Progress] Processed 141000 rows.\n",
      "[Progress] Processed 142000 rows.\n",
      "[Progress] Processed 143000 rows.\n",
      "[Progress] Processed 144000 rows.\n",
      "[Progress] Processed 145000 rows.\n",
      "[Progress] Processed 146000 rows.\n",
      "[Progress] Processed 147000 rows.\n",
      "[Progress] Processed 148000 rows.\n",
      "[Progress] Processed 149000 rows.\n",
      "[Progress] Processed 150000 rows.\n",
      "[Progress] Processed 151000 rows.\n",
      "[Progress] Processed 152000 rows.\n",
      "[Progress] Processed 153000 rows.\n",
      "[Progress] Processed 154000 rows.\n",
      "[Progress] Processed 155000 rows.\n",
      "[Progress] Processed 156000 rows.\n",
      "[Progress] Processed 157000 rows.\n",
      "[Progress] Processed 158000 rows.\n",
      "[Progress] Processed 159000 rows.\n",
      "[Progress] Processed 160000 rows.\n",
      "[Progress] Processed 161000 rows.\n",
      "[Progress] Processed 162000 rows.\n",
      "[Progress] Processed 163000 rows.\n",
      "[Progress] Processed 164000 rows.\n",
      "[Progress] Processed 165000 rows.\n",
      "[Progress] Processed 166000 rows.\n",
      "[Progress] Processed 167000 rows.\n",
      "[Progress] Processed 168000 rows.\n",
      "[Progress] Processed 169000 rows.\n",
      "[Progress] Processed 170000 rows.\n",
      "[Progress] Processed 171000 rows.\n",
      "[Progress] Processed 172000 rows.\n",
      "[Progress] Processed 173000 rows.\n",
      "[Progress] Processed 174000 rows.\n",
      "[Progress] Processed 175000 rows.\n",
      "[Progress] Processed 176000 rows.\n",
      "[Progress] Processed 177000 rows.\n",
      "[Progress] Processed 178000 rows.\n",
      "[Progress] Processed 179000 rows.\n",
      "[Progress] Processed 180000 rows.\n",
      "[Progress] Processed 181000 rows.\n",
      "[Progress] Processed 182000 rows.\n",
      "[Progress] Processed 183000 rows.\n",
      "[Progress] Processed 184000 rows.\n",
      "[Progress] Processed 185000 rows.\n",
      "[Progress] Processed 186000 rows.\n",
      "[Progress] Processed 187000 rows.\n",
      "[Progress] Processed 188000 rows.\n",
      "[Progress] Processed 189000 rows.\n",
      "[Progress] Processed 190000 rows.\n",
      "[Progress] Processed 191000 rows.\n",
      "[Progress] Processed 192000 rows.\n",
      "[Progress] Processed 193000 rows.\n",
      "[Progress] Processed 194000 rows.\n",
      "[Progress] Processed 195000 rows.\n",
      "[Progress] Processed 196000 rows.\n",
      "[Progress] Processed 197000 rows.\n",
      "[Progress] Processed 198000 rows.\n",
      "[Progress] Processed 199000 rows.\n",
      "[Progress] Processed 200000 rows.\n",
      "[Progress] Processed 201000 rows.\n",
      "[Progress] Processed 202000 rows.\n",
      "[Progress] Processed 203000 rows.\n",
      "[Progress] Processed 204000 rows.\n",
      "[Progress] Processed 205000 rows.\n",
      "[Progress] Processed 206000 rows.\n",
      "[Progress] Processed 207000 rows.\n",
      "[Progress] Processed 208000 rows.\n",
      "[Progress] Processed 209000 rows.\n",
      "[Progress] Processed 210000 rows.\n",
      "[Progress] Processed 211000 rows.\n",
      "[Progress] Processed 212000 rows.\n",
      "[Progress] Processed 213000 rows.\n",
      "[Progress] Processed 214000 rows.\n",
      "[Progress] Processed 215000 rows.\n",
      "[Progress] Processed 216000 rows.\n",
      "[Progress] Processed 217000 rows.\n",
      "[Progress] Processed 218000 rows.\n",
      "[Progress] Processed 219000 rows.\n",
      "[Progress] Processed 220000 rows.\n",
      "[Progress] Processed 221000 rows.\n",
      "[Progress] Processed 222000 rows.\n",
      "[Progress] Processed 223000 rows.\n",
      "[Progress] Processed 224000 rows.\n",
      "[Progress] Processed 225000 rows.\n",
      "[Progress] Processed 226000 rows.\n",
      "[Progress] Processed 227000 rows.\n",
      "[Progress] Processed 228000 rows.\n",
      "[Progress] Processed 229000 rows.\n",
      "[Progress] Processed 230000 rows.\n",
      "[Progress] Processed 231000 rows.\n",
      "[Progress] Processed 232000 rows.\n",
      "[Progress] Processed 233000 rows.\n",
      "[Progress] Processed 234000 rows.\n",
      "[Progress] Processed 235000 rows.\n",
      "[Progress] Processed 236000 rows.\n",
      "[Progress] Processed 237000 rows.\n",
      "[Progress] Processed 238000 rows.\n",
      "[Progress] Processed 239000 rows.\n",
      "[Progress] Processed 240000 rows.\n",
      "[Progress] Processed 241000 rows.\n",
      "[Progress] Processed 242000 rows.\n",
      "[Progress] Processed 243000 rows.\n"
     ]
    }
   ],
   "source": [
    "processed_counter = 0\n",
    "scores = data['tweet'].apply(sentiment_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract 3-dim sentiment score and label straight from BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment score and label column\n",
    "data['sentiment_BERT'] = scores\n",
    "data['sentiment_score'] =  data['sentiment_BERT'].apply(lambda row: row[np.argsort(row)[-1]])\n",
    "data['sentiment_label'] = data['sentiment_BERT'].apply(lambda row: 'negative' if np.argsort(row)[-1] == 0 else ('neutral' if np.argsort(row)[-1] == 1 else 'positive'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction\n",
    "- Transform sentiment from 3 dimension to 2 dimension, ranging [0, 1]\n",
    "- Transform sentiment from 3 dimension to 1 dimension, ranging [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment_2dim_label'] = data['sentiment_BERT'].apply(lambda row: 'negative' if row[0] > row[2] else ('positive' if row[0] < row[2] else 'tie'))\n",
    "data['sentiment_2dim_score'] = data['sentiment_BERT'].apply(lambda row: row[0] if row[0] > row[2] else (row[2] if row[0] < row[2] else 0))\n",
    "data['sentiment_1dim'] = data['sentiment_BERT'].apply(lambda row: -row[0] if row[0] > row[2] else (row[2] if row[0] < row[2] else 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine sentiment score with the complete dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join this table with the complete table\n",
    "\n",
    "bigger_file = pd.read_csv(\"Test Data/Filtered_Tweets_Test.csv\")\n",
    "df_new = bigger_file.merge(data[['id','sentiment_BERT','sentiment_score','sentiment_label', 'sentiment_2dim_label', 'sentiment_2dim_score', 'sentiment_1dim']], on='id', how='left')\n",
    "df_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create topic_new category\n",
    "- Re-categorize topic, put topic_score < 0.8 to \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_topic(row):\n",
    "    if row['topic_score'] < 0.8: # 1 standard deviation\n",
    "        return \"Other\"\n",
    "    if row['topic'] == \"news_&_social_concern\":\n",
    "        return \"politics\"\n",
    "    elif row['topic'] == \"diaries_&_daily_life\":\n",
    "        return \"diaries\"\n",
    "    elif row['topic'] == \"sports\":\n",
    "        return \"sports\"\n",
    "    elif row['topic'] in [\"film_tv_&_video\", \"music\"]:\n",
    "        return \"entertainment\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "df_new['topic_new'] = df_new.apply(map_topic, axis=1)\n",
    "df_new['hashtag_bool'] = df_new['hashtag_count'] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save a copy to csv\n",
    "# df_new.reset_index(drop=True).to_csv(\"Test Data/Complete_Test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply filters\n",
    "- filter 'follower_count'\n",
    "- filter 'word_count'\n",
    "- filter out replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_follower_95 = df_new['follower_count'].quantile(0.95)\n",
    "# threshold_likes_95 = df_new['likes'].quantile(0.95)\n",
    "# threshold_retweets_95 = df_new['retweets'].quantile(0.95)\n",
    "# threshold_replies_95 = df_new['replies'].quantile(0.95)\n",
    "# threshold_quotes_95 = df_new['quotes'].quantile(0.95)\n",
    "\n",
    "# Filter based on thresholds\n",
    "df_filtered = df_new[\n",
    "    (df_new['follower_count'] < threshold_follower_95) &\n",
    "    (df_new['follower_count'] > 100) &\n",
    "    (df_new['word_count'] > 2) &\n",
    "    (df_new['word_count'] < 75)]\n",
    "\n",
    "# Filter out replies and create hashtag_bool column\n",
    "df_filtered = df_filtered[\n",
    "    df_filtered['is_reply'] == False\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_filtered.reset_index(drop=True).to_csv(\"Test Data/Complete_Test_data_filtered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new = pd.read_csv(\"English_AllTweet_Sample/complete_270k.csv\", index_col=False).drop(columns=['Unnamed: 0','sentiment_1dim'])\n",
    "# df_new['sentiment_BERT'] = df_new['sentiment_BERT'].apply(lambda row: json.loads(row))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
