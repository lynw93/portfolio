{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41a6a8c-a433-443c-9b05-5d3603c9d915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Setting up topic classification model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using alternative pipeline configuration\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer, \n",
    "    AutoConfig,\n",
    "    pipeline\n",
    ")\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "import re\n",
    "import textstat \n",
    "import emoji\n",
    "\n",
    "\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Try to update/fix transformers installation (optional)\n",
    "# Uncomment these lines if you want to try reinstalling transformers\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install --upgrade transformers accelerate\n",
    "\n",
    "# Set up topic classification model with error handling\n",
    "print(\"Setting up topic classification model...\")\n",
    "TOPIC_MODEL = \"cardiffnlp/tweet-topic-base-multilingual\"\n",
    "try:\n",
    "    topic_tokenizer = AutoTokenizer.from_pretrained(TOPIC_MODEL)\n",
    "    try:\n",
    "        topic_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            TOPIC_MODEL, device_map=\"auto\"\n",
    "        )\n",
    "    except (TypeError, ValueError):\n",
    "        topic_model = AutoModelForSequenceClassification.from_pretrained(TOPIC_MODEL)\n",
    "        topic_model = topic_model.to(device)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading topic model: {e}\")\n",
    "    print(\"Trying alternative loading method...\")\n",
    "    topic_tokenizer = AutoTokenizer.from_pretrained(TOPIC_MODEL)\n",
    "    topic_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        TOPIC_MODEL, ignore_mismatched_sizes=True\n",
    "    )\n",
    "    topic_model = topic_model.to(device)\n",
    "\n",
    "# Create the topic classifier pipeline\n",
    "try:\n",
    "    # Try different device specifications based on version\n",
    "    topic_classifier = pipeline(\n",
    "        \"text-classification\", \n",
    "        model=topic_model, \n",
    "        tokenizer=topic_tokenizer, \n",
    "        top_k=1,\n",
    "        device=0 if device.startswith(\"cuda\") else -1\n",
    "    )\n",
    "except:\n",
    "    # Fallback if the above doesn't work\n",
    "    print(\"Using alternative pipeline configuration\")\n",
    "    topic_classifier = pipeline(\n",
    "        \"text-classification\", \n",
    "        model=topic_model, \n",
    "        tokenizer=topic_tokenizer, \n",
    "        top_k=1\n",
    "    )\n",
    "\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in str(text).split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def get_tweet_topic_with_score(text):\n",
    "    try:\n",
    "        result = topic_classifier(text)\n",
    "        top_result = result[0][0]\n",
    "        return (top_result[\"label\"], top_result[\"score\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text[:50]}... Error: {e}\")\n",
    "        return (\"error\", 0.0)\n",
    "\n",
    "def textstat_readability(text):\n",
    "    return textstat.flesch_kincaid_grade(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb2f9774-a62c-4beb-a2f2-e8ecdaf34feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_tweets = pd.read_csv(\"Filtered_Tweets_english_1.csv\")\n",
    "existing_texts = set(existing_tweets[\"tweet\"].astype(str).str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7014249c-c4cc-4594-9fed-873639a6fc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading users dataset to identify verified users with high follower counts...\n",
      "Found 106898 users with over 100 followers\n",
      "Now filtering tweets dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcd7c8c60db4912a51c0ec118fcb0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5000000 total tweets, found 127402 eligible tweets\n",
      "Processed 10000000 total tweets, found 245914 eligible tweets\n",
      "Processed 15000000 total tweets, found 375055 eligible tweets\n",
      "Processed 20000000 total tweets, found 505844 eligible tweets\n",
      "Processed 25000000 total tweets, found 631348 eligible tweets\n",
      "Processed 30000000 total tweets, found 762494 eligible tweets\n",
      "Processed 35000000 total tweets, found 889592 eligible tweets\n",
      "Processed 40000000 total tweets, found 1021738 eligible tweets\n",
      "Processed 45000000 total tweets, found 1136438 eligible tweets\n",
      "Processed 50000000 total tweets, found 1252888 eligible tweets\n",
      "Processed 55000000 total tweets, found 1380107 eligible tweets\n",
      "Processed 60000000 total tweets, found 1501550 eligible tweets\n",
      "Processed 65000000 total tweets, found 1626450 eligible tweets\n",
      "Processed 70000000 total tweets, found 1751556 eligible tweets\n",
      "Processed 75000000 total tweets, found 1879238 eligible tweets\n",
      "Processed 80000000 total tweets, found 2010218 eligible tweets\n",
      "Processed 85000000 total tweets, found 2139197 eligible tweets\n"
     ]
    }
   ],
   "source": [
    "# First, load and process the users dataset to identify verified users with 10k+ followers\n",
    "print(\"Loading users dataset to identify verified users with high follower counts...\")\n",
    "users_dataset = load_dataset(\"enryu43/twitter100m_users\", split=\"train\", streaming=True)\n",
    "\n",
    "# Create a dictionary to store eligible user IDs and their follower counts\n",
    "eligible_users = {}\n",
    "\n",
    "# Stream through users dataset to find verified users with 100k+ followers\n",
    "for row in users_dataset:\n",
    "    if row.get(\"followers\", 0) > 100:\n",
    "        eligible_users[row[\"user\"]] = row[\"followers\"]\n",
    "\n",
    "print(f\"Found {len(eligible_users)} users with over 100 followers\")\n",
    "\n",
    "# Now load and filter tweets dataset\n",
    "print(\"Now filtering tweets dataset...\")\n",
    "tweets_stream = load_dataset(\"enryu43/twitter100m_tweets\", split=\"train\", streaming=True)\n",
    "\n",
    "# Initialize reservoir sampling\n",
    "sample_size = 500000\n",
    "reservoir = []\n",
    "count = 0\n",
    "processed = 0\n",
    "seen_tweets = set()  # Track unique tweets to avoid duplicates\n",
    "\n",
    "# Stream and filter on-the-fly\n",
    "for row in tweets_stream:\n",
    "    processed += 1\n",
    "    if processed % 5000000 == 0:\n",
    "        print(f\"Processed {processed} total tweets, found {count} eligible tweets\")\n",
    "    \n",
    "    # First check that text field exists and is not empty\n",
    "    if not row.get(\"tweet\") or str(row.get(\"tweet\")).strip() == \"\":\n",
    "        continue\n",
    "    \n",
    "    # Check for any other null values in important fields\n",
    "    if row.get(\"user\") is None or row.get(\"date\") is None:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    # Check if we've already seen this tweet text\n",
    "    tweet_text = str(row.get(\"tweet\")).strip()\n",
    "    if tweet_text in seen_tweets or tweet_text in existing_texts:\n",
    "        continue  # Skip duplicate tweets\n",
    "    \n",
    "    # Calculate engagement score\n",
    "    engagement_score = (row.get(\"likes\", 0) + \n",
    "                        row.get(\"replies\", 0) + \n",
    "                        row.get(\"retweets\", 0) + \n",
    "                        row.get(\"quotes\", 0))\n",
    "\n",
    "    # Create a copy of the row and add the engagement score\n",
    "    row_copy = dict(row)\n",
    "    row_copy[\"engagement_score\"] = engagement_score\n",
    "        \n",
    "    # Check all conditions before considering for the reservoir:\n",
    "    # 1. From March 2023\n",
    "    # 2. Has engagement score > 1\n",
    "    # 3. From a verified user with 10k+ followers\n",
    "    if (\"date\" in row and \n",
    "        row[\"date\"].startswith(\"2023-03\") and\n",
    "        row[\"user\"] in eligible_users and\n",
    "        engagement_score > 1 and \"http\" not in row[\"tweet\"]):\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        if len(reservoir) < sample_size:\n",
    "            reservoir.append(row_copy)  # Fill up the reservoir first\n",
    "            seen_tweets.add(tweet_text)  # Mark this tweet as seen\n",
    "        else:\n",
    "            # Replace random items in the reservoir with decreasing probability\n",
    "            replace_idx = random.randint(0, count - 1)\n",
    "            if replace_idx < sample_size:\n",
    "                # Remove the old tweet text from seen_tweets\n",
    "                old_tweet_text = str(reservoir[replace_idx].get(\"tweet\")).strip()\n",
    "                seen_tweets.discard(old_tweet_text)\n",
    "                \n",
    "                # Add the new tweet\n",
    "                reservoir[replace_idx] = row_copy\n",
    "                seen_tweets.add(tweet_text)  # Mark this tweet as seen\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "tweets = pd.DataFrame(reservoir)\n",
    "\n",
    "# Final quality check - remove any rows with NaN values in critical columns\n",
    "tweets = tweets.dropna(subset=[\"tweet\", \"user\", \"date\"])\n",
    "\n",
    "# Add follower count to each row\n",
    "tweets[\"follower_count\"] = tweets[\"user\"].map(eligible_users)\n",
    "\n",
    "# Calculate average engagement per user\n",
    "user_avg_engagement = tweets.groupby(\"user\")[\"engagement_score\"].mean().reset_index()\n",
    "user_avg_engagement.rename(columns={\"engagement_score\": \"avg_engagement\"}, inplace=True)\n",
    "\n",
    "# Merge the average engagement back into the main DataFrame\n",
    "tweets = pd.merge(tweets, user_avg_engagement, on=\"user\", how=\"left\")\n",
    "\n",
    "tweets.to_csv('checkpoint1_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba9b862a-b4e5-414c-88b7-e608e731963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500000/500000 [00:00<00:00, 976216.40it/s] \n",
      "100%|██████████| 500000/500000 [00:00<00:00, 585749.04it/s]\n",
      "100%|██████████| 500000/500000 [00:00<00:00, 1155219.51it/s]\n",
      "100%|██████████| 500000/500000 [00:00<00:00, 1150179.40it/s]\n",
      "100%|██████████| 500000/500000 [00:00<00:00, 1424647.26it/s]\n",
      "100%|██████████| 500000/500000 [00:02<00:00, 171100.10it/s]\n",
      "100%|██████████| 500000/500000 [18:37<00:00, 447.53it/s]\n",
      "100%|██████████| 243399/243399 [00:20<00:00, 11676.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sample contains 243399 tweets from verified and unverified users with >100 followers\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()  # Enable the progress_apply method\n",
    "\n",
    "# Add tweet character length \n",
    "tweets[\"characters\"] = tweets[\"tweet\"].progress_apply(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "# Add tweet word count\n",
    "tweets[\"word_count\"] = tweets[\"tweet\"].progress_apply(lambda x: len(str(x).split()))\n",
    "# Add hashtag count\n",
    "tweets[\"hashtag_count\"] = tweets[\"tweet\"].progress_apply(lambda x: str(x).count(\"#\"))\n",
    "# Add mention count \n",
    "tweets[\"mention_count\"] = tweets[\"tweet\"].progress_apply(lambda x: str(x).count(\"@\"))\n",
    "# Add a bool for if it has a url (we can also filter this out if we want) \n",
    "tweets[\"has_url\"] = tweets[\"tweet\"].progress_apply(lambda x: \"http\" in str(x))\n",
    "# Create a new column 'is_reply' where True if tweet starts with '@', else False\n",
    "tweets['is_reply'] = tweets['tweet'].str.startswith('@', na=False)\n",
    "# Add emoji count \n",
    "import emoji\n",
    "tweets[\"emoji_count\"] = tweets[\"tweet\"].progress_apply(lambda x: sum(1 for char in str(x) if char in emoji.EMOJI_DATA))\n",
    "# Function to remove hashtags and mentions before we detect the language \n",
    "def clean_text(text):\n",
    "    # Remove mentions (anything starting with @) and hashtags (anything starting with #)\n",
    "    text = re.sub(r'@\\w+', '', text)  # Removes mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # Removes hashtags\n",
    "    return text\n",
    "# Function to detect language\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        cleaned_text = clean_text(text)  # Clean the tweet text\n",
    "        return detect(cleaned_text)  # Detect language from cleaned text\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "# Apply language detection to each tweet\n",
    "tweets[\"language\"] = tweets[\"tweet\"].progress_apply(detect_language)\n",
    "# ISO 639-1 language code to full name\n",
    "lang_map = {\n",
    "    \"af\": \"Afrikaans\",\n",
    "    \"ar\": \"Arabic\",\n",
    "    \"bg\": \"Bulgarian\",\n",
    "    \"bn\": \"Bengali\",\n",
    "    \"ca\": \"Catalan\",\n",
    "    \"cs\": \"Czech\",\n",
    "    \"cy\": \"Welsh\",\n",
    "    \"da\": \"Danish\",\n",
    "    \"de\": \"German\",\n",
    "    \"el\": \"Greek\",\n",
    "    \"en\": \"English\",\n",
    "    \"es\": \"Spanish\",\n",
    "    \"et\": \"Estonian\",\n",
    "    \"fa\": \"Persian (Farsi)\",\n",
    "    \"fi\": \"Finnish\",\n",
    "    \"fr\": \"French\",\n",
    "    \"gu\": \"Gujarati\",\n",
    "    \"he\": \"Hebrew\",\n",
    "    \"hi\": \"Hindi\",\n",
    "    \"hr\": \"Croatian\",\n",
    "    \"hu\": \"Hungarian\",\n",
    "    \"id\": \"Indonesian\",\n",
    "    \"it\": \"Italian\",\n",
    "    \"ja\": \"Japanese\",\n",
    "    \"kn\": \"Kannada\",\n",
    "    \"ko\": \"Korean\",\n",
    "    \"lt\": \"Lithuanian\",\n",
    "    \"lv\": \"Latvian\",\n",
    "    \"mk\": \"Macedonian\",\n",
    "    \"ml\": \"Malayalam\",\n",
    "    \"mr\": \"Marathi\",\n",
    "    \"ne\": \"Nepali\",\n",
    "    \"nl\": \"Dutch\",\n",
    "    \"no\": \"Norwegian\",\n",
    "    \"pa\": \"Punjabi\",\n",
    "    \"pl\": \"Polish\",\n",
    "    \"pt\": \"Portuguese\",\n",
    "    \"ro\": \"Romanian\",\n",
    "    \"ru\": \"Russian\",\n",
    "    \"sk\": \"Slovak\",\n",
    "    \"sl\": \"Slovenian\",\n",
    "    \"so\": \"Somali\",\n",
    "    \"sq\": \"Albanian\",\n",
    "    \"sv\": \"Swedish\",\n",
    "    \"sw\": \"Swahili\",\n",
    "    \"ta\": \"Tamil\",\n",
    "    \"te\": \"Telugu\",\n",
    "    \"th\": \"Thai\",\n",
    "    \"tl\": \"Tagalog\",\n",
    "    \"tr\": \"Turkish\",\n",
    "    \"uk\": \"Ukrainian\",\n",
    "    \"ur\": \"Urdu\",\n",
    "    \"vi\": \"Vietnamese\",\n",
    "    \"zh-cn\": \"Chinese (Simplified)\",\n",
    "    \"zh-tw\": \"Chinese (Traditional)\",\n",
    "    \"unknown\": \"Unknown\"\n",
    "}\n",
    "# Map the codes to full language names\n",
    "tweets[\"language_full\"] = tweets[\"language\"].map(lang_map).fillna(\"No_map\")\n",
    "# Filter to only keep English tweets\n",
    "tweets = tweets[tweets[\"language\"] == \"en\"].reset_index(drop=True)\n",
    "tweets['textstat_readability'] = tweets['tweet'].progress_apply(textstat_readability)\n",
    "# Final DataFrame now has engagement_score, follower_count, avg_engagement, sentiment scores and topic\n",
    "print(f\"Final sample contains {len(tweets)} tweets from verified and unverified users with >100 followers\")\n",
    "tweets.head()\n",
    "tweets.to_csv('checkpoint2_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6660cc6c-f66e-4643-b18d-cc76489a9a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running topic classification on all tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 243399/243399 [2:50:44<00:00, 23.76it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Run topic classification on all tweets\n",
    "print(\"Running topic classification on all tweets...\")\n",
    "# We'll use progress_apply to show a progress bar during processing\n",
    "tweets[[\"topic\", \"topic_score\"]] = tweets[\"tweet\"].progress_apply(\n",
    "    get_tweet_topic_with_score\n",
    ").apply(pd.Series)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06aebe30-1140-4b32-9112-48699b9544f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('Filtered_Tweets_Test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50ad4bf1-2fc6-407e-989f-ee4ba4dd97c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[[\"id\", \"tweet\"]].to_csv(\"tweet_only_Test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3829ad9b-866e-4ac7-ad5b-cd84c19058de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
